<script src="https://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<link href="https://fonts.googleapis.com/css2?family=Open+Sans&display=swap"
      rel="stylesheet">
<link rel="stylesheet" type="text/css" href="./resources/style.css" media="screen"/>

<html lang="en">
<head>
	<title>De-rendering the World's Revolutionary Artefacts</title>
    <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/
        if you update and want to force Facebook to re-scrape. -->
	<meta property="og:image" content="https://sorderender.github.io/resources/og_image.jpg"/>
	<meta property="og:title" content="De-rendering the World's Revolutionary Artefacts." />
	<meta property="og:description" content="We present a model that learns to de-render a single image into shape, albedo and complex lighting and material components." />
    <!-- Twitter automatically scrapes this. Go to https://cards-dev.twitter.com/validator?
        if you update and want to force Twitter to re-scrape. -->
    <meta property="twitter:card"          content="summary" />
    <meta property="twitter:title"         content="De-rendering the World's Revolutionary Artefacts." />
    <meta property="twitter:description"   content="We present a model that learns to de-render a single image into shape, albedo and complex lighting and material components." />
    <meta property="twitter:image"         content="https://sorderender.github.io/resources/og_image.jpg" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Add your Google Analytics tag here -->
    <script async
            src="https://www.googletagmanager.com/gtag/js?id=G-673R9H38J1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());
        gtag('config', 'G-673R9H38J1');
    </script>

</head>

<body>
<div class="container">
    <video autoplay loop muted playsinline width="1200" style="max-width: 100%">
      <source src="resources/vases_row.mp4" type="video/mp4" alt="Met vase results">
    </video>

    <br><br>

    <div class="title">
        De-rendering the World’s Revolutionary Artefacts
    </div>

    <div class="venue">
        In CVPR 2021
    </div>

    <br><br>

    <div class="author">
        <a href="https://elliottwu.com/">Shangzhe Wu</a><sup>1,4*</sup>
    </div>
    <div class="author">
        <a href="https://ameeshmakadia.com/index.html">Ameesh Makadia</a><sup>4</sup>
    </div>
    <div class="author">
        <a href="https://jiajunwu.com/">Jiajun Wu</a><sup>2</sup>
    </div>
    <div class="author">
        <a href="https://www.cs.cornell.edu/~snavely/">Noah Snavely</a><sup>4</sup>
    </div>
    <div class="author">
        <a href="https://research.google/people/RichardTucker/">Richard Tucker</a><sup>4</sup>
    </div>
    <div class="author">
        <a href="https://people.eecs.berkeley.edu/~kanazawa/">Angjoo Kanazawa</a><sup>3,4</sup>
    </div>

    <br><br>

    <div class="affiliation"><sup>1&nbsp;</sup>University of Oxford</div>
    <div class="affiliation"><sup>2&nbsp;</sup>Stanford University</div>
    <div class="affiliation"><sup>3&nbsp;</sup>University of California, Berkeley</div>
    <div class="affiliation"><sup>4&nbsp;</sup>Google Research</div>

    <br><br>

    <div class="links"><a href="https://arxiv.org/abs/2104.03954">[Paper]</a></div>
    <div class="links"><a href="https://youtu.be/pxkYyyw02H0">[Video]</a></div>
    <div class="links"><a href="https://github.com/elliottwu/sorderender">[Code]</a></div>

    <br><br>

    <img style="width: 80%;" src="./resources/teaser.jpg" alt="Teaser figure."/>
    <br><br>
    <p style="width: 80%;">
        Given only a real single-view image collection of “revolutionary” (i.e., solid of revolution) artefacts with known silhouettes as training data (left), our framework learns to de-render a single image into shape, albedo and complex lighting and material components, allowing for novel-view synthesis and relighting (right).
    </p>
    <p style="width: 80%; font-size:14px"> * This work was primarily done while Shangzhe Wu was interning at Google Research.</p>

    <br><br>
    <hr>

    <h1>Abstract</h1>
    <p style="width: 80%;">
        Recent works have shown exciting results in unsupervised image de-rendering—learning to decompose 3D shape, appearance, and lighting from single-image collections without explicit supervision. However, many of these assume simplistic material and lighting models. We propose a method, termed <b><i>RADAR</i></b><sup>†</sup> (<b><i>R</i></b>evolutionary <b><i>A</i></b>rtefact <b><i>D</i></b>e-rendering <b><i>A</i></b>nd <b><i>R</i></b>e-rendering), that can recover environment illumination and surface materials from real single-image collections, relying neither on explicit 3D supervision, nor on multi-view or multi-light images. Specifically, we focus on rotationally symmetric artefacts that exhibit challenging surface properties including specular reflections, such as vases. We introduce a novel self-supervised albedo discriminator, which allows the model to recover plausible albedo without requiring any ground-truth during training. In conjunction with a shape reconstruction module exploiting rotational symmetry, we present an end-to-end learning framework that is able to de-render the world's revolutionary artefacts. We conduct experiments on a real vase dataset and demonstrate compelling decomposition results, allowing for applications including free-viewpoint rendering and relighting.
    </p>
    <p style="width: 80%; font-size:14px"><sup>†</sup> Note the name itself is a "revolutionary" palindrome.</p>

    <br><br>
    <hr>

    <h1>Video</h1>
    <div class="video-container">
        <iframe src="https://www.youtube.com/embed/pxkYyyw02H0" frameBorder="0"
                allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                allowfullscreen></iframe>
    </div>

    <br><br>
    <hr>

    <!-- <h1>Method Overview</h1>
    <img style="width:80%;" src="./resources/method.jpg"
         alt="Method overview figure"/>
    <br>
    <a class="links" href="https://github.com/elliottwu/webpage-template">[Code]</a>

    <br><br>
    <hr> -->

    <h1>Results</h1>
    <h3 style="text-align: center;">De-rendering from a single image</h3>
    <img style="width: 80%;" src="./resources/decom_results/decomposition_01.jpg" alt="Decomposition results 1"/>
    <img style="width: 80%;" src="./resources/decom_results/decomposition_02.jpg" alt="Decomposition results 2"/>
    <img style="width: 80%;" src="./resources/decom_results/decomposition_03.jpg" alt="Decomposition results 3"/>
    <img style="width: 80%;" src="./resources/decom_results/decomposition_04.jpg" alt="Decomposition results 4"/>
    <img style="width: 80%;" src="./resources/decom_results/decomposition_05.jpg" alt="Decomposition results 5"/>
    <img style="width: 80%;" src="./resources/decom_results/decomposition_06.jpg" alt="Decomposition results 6"/>
    <img style="width: 80%;" src="./resources/decom_results/decomposition_07.jpg" alt="Decomposition results 7"/>
    <img style="width: 80%;" src="./resources/decom_results/decomposition_08.jpg" alt="Decomposition results 8"/>
    <img style="width: 80%;" src="./resources/decom_results/decomposition_labels.jpg" alt="Decomposition results labels"/>
    <br><br>
    <h3 style="text-align: center;">Novel view rendering and relighting from a single image</h3>
    <video autoplay loop muted playsinline width="1200" style="max-width: 80%">
      <source src="resources/met_vases.mp4" type="video/mp4" alt="Met vase results">
    </video>
    <p style="text-align: center;">images from <a href="https://metmuseum.github.io/">The Metropolitan Museum of Art Collection</a></p>
    <br><br>
    <video autoplay loop muted playsinline width="1200" style="max-width: 80%">
      <source src="resources/openimages_vases.mp4" type="video/mp4" alt="Open Images vase results">
    </video>
    <p style="text-align: center;">images from <a href="https://storage.googleapis.com/openimages/web/visualizer/index.html?set=train&type=segmentation&r=false&c=%2Fm%2F02s195">Open Images Dataset</a></p>

    <br><br>
    <hr>

    <h1>Paper</h1>
    <div class="paper-thumbnail">
        <a href="https://arxiv.org">
            <img class="layered-paper-big" width="100%" src="./resources/paper.jpg" alt="Paper thumbnail."/>
        </a>
    </div>
    <div class="paper-info">
        <h3>De-rendering the World's Revolutionary Artefacts</h3>
        <p>Shangzhe Wu, Ameesh Makadia, Jiajun Wu, Noah Snavely, Richard Tucker, and Angjoo Kanazawa</p>
        <p>In CVPR 2021</p>
        <pre><code>@InProceedings{wu2021derender,
    author={Shangzhe Wu and Ameesh Makadia and Jiajun Wu and Noah Snavely and Richard Tucker and Angjoo Kanazawa},
    title={De-rendering the World's Revolutionary Artefacts},
    booktitle = {CVPR},
    year = {2021}
}</code></pre>
    </div>

    <br><br>
    <hr>

    <h1>Acknowledgements</h1>
    <p style="width: 80%;">
        We would like to thank Christian Rupprecht, Soumyadip Sengupta, Manmohan Chandraker and Andrea Vedaldi for insightful discussions.
        The <a href="https://github.com/elliottwu/webpage-template">webpage template</a> was adapted from <a href="https://github.com/richzhang/webpage-template">Richard Zhang's</a> and <a href="https://github.com/jasonyzhang/webpage-template">Jason Zhang's</a> templates.
    </p>

    <br><br>
</div>

</body>

</html>
